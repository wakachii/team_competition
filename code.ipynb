{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "importとか諸々"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "データの読み込み"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "year1 = pd.read_csv('year1.csv')\n",
    "year2 = pd.read_csv('year2.csv')\n",
    "year3 = pd.read_csv('year2.csv')\n",
    "year4 = pd.read_csv('year2.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "year1[\"year\"] = 1\n",
    "year2[\"year\"] = 2\n",
    "year3[\"year\"] = 3\n",
    "year4[\"year\"] = 4\n",
    "test[\"year\"] = 5\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "データをまとめる"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "all_data = pd.concat([year1,year2,year3,year4])\n",
    "all_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "前処理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "drop_col = [\"TARGET\",\"id\"]\n",
    "for i in all_data.drop(drop_col,axis=1):\n",
    "    all_data[i] = pd.to_numeric(all_data[i],errors='coerce')#\"？\"（非数値型）をNaNに置換\n",
    "all_data = all_data.fillna(all_data.mean())#NaNを中央値に置換"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "for i in test.drop(\"id\",axis=1):\n",
    "    test[i] = pd.to_numeric(test[i],errors='coerce')\n",
    "test = test.fillna(test.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "特徴量を増やす"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "#total_assetsを真数に\n",
    "all_data[\"total_assets\"] = np.exp(all_data[\"X29\"])\n",
    "#分母がtotal_assetsの変数にかけて特徴量を増やす\n",
    "all_data[\"net_profit\"] = all_data[\"total_assets\"] * all_data[\"X1\"]\n",
    "all_data[\"total liabilities\"] = all_data[\"total_assets\"] * all_data[\"X2\"]\n",
    "all_data[\"total liabilities\"] = all_data[\"total_assets\"] * all_data[\"X3\"]\n",
    "all_data[\"retained earnings\"] = all_data[\"total_assets\"] * all_data[\"X6\"]\n",
    "all_data[\"gross profit + interest\"] = all_data[\"total_assets\"] * all_data[\"X14\"]\n",
    "all_data[\"gross profit\"] = all_data[\"total_assets\"] * all_data[\"X18\"]\n",
    "all_data[\"gross profit (in 3 years)\"] = all_data[\"total_assets\"] * all_data[\"X24\"]\n",
    "all_data[\"(equity - share capital)\"] = all_data[\"total_assets\"] * all_data[\"X25\"]\n",
    "all_data[\"profit on sales\"] = all_data[\"total_assets\"] * all_data[\"X35\"]\n",
    "all_data[\"total sales\"] = all_data[\"total_assets\"] * all_data[\"X36\"]\n",
    "all_data[\"constant capital\"] = all_data[\"total_assets\"] * all_data[\"X38\"]\n",
    "all_data[\"EBITDA\"] = all_data[\"total_assets\"] * all_data[\"X48\"]\n",
    "all_data[\"short-term liabilities\"] = all_data[\"total_assets\"] * all_data[\"X51\"]\n",
    "\n",
    "#testデータも同様に処理\n",
    "test[\"total_assets\"] = np.exp(test[\"X29\"])\n",
    "\n",
    "test[\"net_profit\"] = test[\"total_assets\"] * test[\"X1\"]\n",
    "test[\"total liabilities\"] = test[\"total_assets\"] * test[\"X2\"]\n",
    "test[\"total liabilities\"] = test[\"total_assets\"] * test[\"X3\"]\n",
    "test[\"retained earnings\"] = test[\"total_assets\"] * test[\"X6\"]\n",
    "test[\"gross profit + interest\"] = test[\"total_assets\"] * test[\"X14\"]\n",
    "test[\"gross profit\"] = test[\"total_assets\"] * test[\"X18\"]\n",
    "test[\"gross profit (in 3 years)\"] = test[\"total_assets\"] * test[\"X24\"]\n",
    "test[\"(equity - share capital)\"] = test[\"total_assets\"] * test[\"X25\"]\n",
    "test[\"profit on sales\"] = test[\"total_assets\"] * test[\"X35\"]\n",
    "test[\"total sales\"] = test[\"total_assets\"] * test[\"X36\"]\n",
    "test[\"constant capital\"] = test[\"total_assets\"] * test[\"X38\"]\n",
    "test[\"EBITDA\"] = test[\"total_assets\"] * test[\"X48\"]\n",
    "test[\"short-term liabilities\"] = test[\"total_assets\"] * test[\"X51\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "取り除く列のリスト\n",
    "not_use_columns = ['X37']\n",
    "# 学習用データから列を削除する（PassengerIdは後ほど取り除く）\n",
    "all_data.drop(not_use_columns, axis=1, inplace=True)\n",
    "# テスト用データから列を削除する\n",
    "test.drop(not_use_columns, axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Xとyの決定"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "X_1 = all_data.drop(drop_col,axis=1)\n",
    "y_1 = all_data.loc[:, 'TARGET']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X_1)\n",
    "\n",
    "X_1_train, X_1_test, y_1_train, y_1_test = train_test_split(X_1, y_1, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pca = PCA(n_components = 20)\n",
    "X_pca = pca.fit_transform(all_data)\n",
    "\n",
    "distortions = []\n",
    "\n",
    "for k  in range(1,20):             \n",
    "    km = KMeans(n_clusters=k,       \n",
    "                init='k-means++',\n",
    "                n_init=10,        \n",
    "                max_iter=300,    \n",
    "                random_state=0) \n",
    "    km.fit(X_pca)           \n",
    "    distortions.append(km.inertia_) \n",
    "\n",
    "# 結果を散布図に出力\n",
    "\n",
    "plt.plot(range(1,20), distortions,marker='o') \n",
    "plt.xticks([i for i in range(1, 20)])         \n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=80, max_features='auto')\n",
    "rf.fit(X_rf_train, y_rf_train)\n",
    "print('Training done using Random Forest')\n",
    "ranking = np.argsort(-rf.feature_importances_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training done using Random Forest\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "cols = X_1.columns.values[ranking][0:40]\n",
    "X = all_data[cols]\n",
    "test_X = test[cols]\n",
    "y = all_data.loc[:, 'TARGET']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.3,random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "params_xgb = {'max_depth': 4,\n",
    "              'eta': 0.3,\n",
    "              'objective': 'binary:logistic',\n",
    "              'silent':1,\n",
    "              'random_state':1234, \n",
    "              'eval_metric': 'rmse'\n",
    "             } \n",
    "watch_list = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "model_xgb = xgb.train(params_xgb,\n",
    "                      dtrain,\n",
    "                      early_stopping_rounds=20,\n",
    "                      evals=watch_list)\n",
    "\n",
    "xgb_predict = model_xgb.predict(xgb.DMatrix(test_X), ntree_limit = model_xgb.best_ntree_limit)\n",
    "\n",
    "# データを整形\n",
    "xgb_predict = np.array([[1-i, i] for i in xgb_predict])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from catboost import CatBoost, CatBoostClassifier\n",
    "from catboost import Pool\n",
    "\n",
    "#データをセット\n",
    "catBoost_train = Pool(X_train, label=y_train) \n",
    "catBoost_eval = Pool(X_test, label=y_test)\n",
    "\n",
    "# パラメータを設定\n",
    "params_cat = {'loss_function':'Logloss',\n",
    "              'num_boost_round':1000,\n",
    "              'early_stopping_rounds':20,\n",
    "             } \n",
    "# 学習 \n",
    "catb = CatBoost(params_cat)\n",
    "catb.fit(catBoost_train, eval_set=catBoost_eval ,use_best_model=True, verbose=False)\n",
    "cat_predict = catb.predict(test_X, prediction_type='Probability') \n",
    "\n",
    "cat_predict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "\n",
    "light_params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'learning_rate': 0.1, \n",
    "                'num_leaves': 40 \n",
    "               } \n",
    "\n",
    "evaluation_results = {}\n",
    "#学習 \n",
    "lgb_model = lgb.train(light_params, # 上記で設定したパラメータ\n",
    "                      lgb_train, # 使用するデータセット \n",
    "                      num_boost_round=200, # 学習の回数 \n",
    "                      valid_names=['train', 'valid'], # 学習経過で表示する名称 \n",
    "                      valid_sets=[lgb_train, lgb_eval], # モデルの検証に使用するデータセット \n",
    "                      evals_result=evaluation_results, # 学習の経過を保存 \n",
    "                      early_stopping_rounds=20, # アーリーストッピングの回数 \n",
    "                      verbose_eval=0) # 学習の経過を表示する刻み（非表示）  \n",
    "\n",
    "# テストデータで予測\n",
    "lgb_predict = lgb_model.predict(test_X, num_iteration=lgb_model.best_iteration)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "k1 = 1 \n",
    "k2 = 1 \n",
    "k3 = 1 \n",
    "\n",
    "voting_result = [] \n",
    "\n",
    "for i in range(len(test)):\n",
    "    voting = np.argmax(k1*xgb_predict[i] + k2*cat_predict[i] + k3*lgb_predict[i])\n",
    "    np.round(voting).astype(int) \n",
    "    voting_result.append(voting)\n",
    "    prediction_XG = np.round(voting_result).astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred = prediction_XG"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission = pd.DataFrame({'id': test['id'], 'TARGET': pred})\n",
    "submission.to_csv('submission.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "保存"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''from sklearn.linear_model import LogisticRegression\n",
    "models = {\n",
    "    (\"RandomForest\", RandomForestClassifier()),\n",
    "    (\"GradientBoosting\", GradientBoostingClassifier()),\n",
    "    (\"NeuralNetwork\", MLPClassifier(max_iter = 10000,)),\n",
    "    (\"AdaBoost\", AdaBoostClassifier())\n",
    "    }\n",
    "for name, model in models:\n",
    "    acc = accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test))\n",
    "    print(\"Accuracy of\", name, \":\", acc)\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "stacking = StackingClassifier(estimators = models, final_estimator = MLPClassifier(), n_jobs = cores)\n",
    "acc = accuracy_score(y_test, stacking.fit(X_train, y_train).predict(X_test))\n",
    "print(\"Accuracy of Stacking :\", acc)\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "csvに出力"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('anaconda3-5.3.1': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "interpreter": {
   "hash": "c169cd767ee046a038f135e031ddb77626caae741064c89d221e3e4459bd2ed0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}